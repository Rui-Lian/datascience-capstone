---
title: 'Capstone Project: Milestone Report Week 2'
author: "Rui Lian"
date: "21 December 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=TRUE, echo=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```


##  1. Objectives. 
- Getting and cleaning data; 
- Exploratory analysis;  
- Create fundations for further exploration and modeling. 

##  2. Prerequisite and data preparation. 
The Rmarkdown file can be found at: https://github.com/Rui-Lian/datascience-capstone.git

The following packages are used for this report. 
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)# A set of hand-on packages from Rstudio. 
library(tidytext)# Text mining package using tidy data principle.
library(wordcloud)# Word cloud for exploration.
library(quanteda)# Create document frequency matrix.
library(stringi) # Regrex to clean the word strings. 
```
  Two NLP packages are used for this project. 
  
  Package tidytext (https://cran.r-project.org/web/packages/tidytext/index.html) adopts tidy data priciple. So we can treat text data as a data frame and we can use all of the previous tools such as ggplot2, dplyr etc. 

  Package quanteda is a powerful, convenient package for NLP. But it takes some time to be familiar with it. 

###  2.1 Getting the data. 
  Raw data was downloaded from "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip". 
  The zip file was unziped in R with unzip funciton. 
  All the data ins in a folder named final in the working directory. 
```{r, echo=FALSE, eval=FALSE, include=FALSE}
# create file URL. 
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

# create the data file path
dataPath <- paste(getwd(),"/raw.zip", sep = "")

dataPath
# download the data
download.file(fileUrl,
              destfile = dataPath,
              method = "curl")

unzip(dataPath)
```

   Three text files are downloaded into `final` folder in working directory. These files represent data from twitter, news and blog, respectively. 
   25% ramdom samples of the three files was read into a data frame by base::scan. 
```{r, cache=TRUE, echo=FALSE, eval=TRUE, include=TRUE}
# List files in final/en_US folder. 
files <- list.files("../capstone/final/en_US/")
raw <- NULL ## raw as the collection of threecorpus.
for (i in 1:3){
        set.seed(38360) # set seed
        # create file path
        path <- paste(getwd(), "/final", "/en_US/", 
                files[i], sep = "")
        # create connection by path. 
        con <- file(path, "r")
        # scan and read data. skip lines with NULL. 
        dat <- scan(con, what = "character", 
                       sep = "\n", skipNul = TRUE)
        # create random selection by rninom
        select <- rbinom(length(dat), size = 1, prob = 0.25)
        # subsetting each corpus. 
        dat <- dat[select == 1]
        # create a data frame from sample of each corpus. 
        # 1=blog, 2=news, 3=twitter
        dat <- data_frame(text = dat, source = as.character(files[i]))
        # combine the sample to data frame raw
        raw <- rbind(raw, dat)
        # close the connection and loop again. 
        close(con)
        rm(dat)
        rm(select)
}

raw$text <- str_to_lower(raw$text)
```

  As the result: a data frame of more than 1.06 million  rows was generated. The size of this sampled raw data is 208 M. 
```{r, echo=FALSE, cache=TRUE, eval=TRUE, include=FALSE}
object.size(raw)
```


### 2.2 Data cleaning-profanity removing.
  A "bad-word" list was downloaded from "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt". 

  Each word in the list was pasted as a regular expression like "\\bbad\\b". As such, we only detect STANDALONE bad words in the corpus. It is possible that there are hidden bad words in a string. 

```{r, eval=TRUE, echo=FALSE, cache=TRUE}
# Creating the CMU bad word list in R.
## URL. 
fileUrl <- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"

## Local file path.
badwPath <- paste(getwd(),"/bad.txt", sep = "")

## Download the data. 
download.file(fileUrl,
              destfile = badwPath,
              method = "curl")

## Read bad.text into R.
con <- file(badwPath, "r")
bad <- scan(con, what = "character",sep = "\n")
close(con) 
## total 1383 bad words as string vector. 

## Regrex to define the word boundary. 
bad <- paste("\\b", bad, "\\b", sep = "")

## Combine the word list to a string with alteration "|". 
bad <- str_c(bad, collapse = "|")
```


## 3. Tokenization and feature exploration.
  One word tokenization was performed by tidytext::unnest_tokens function. 
  Source of the text, twitter, news and blogs are labeled for each row. 
  There are more than 25 million one-word tokens, representing more than 310 K unique words or symbols. The common stop words, such as "the", "of" etc, are not excluded because predictions of these word could make the typing more easy, especially in mobile settings. 
  A example of the first two rows of the data frame. 
```{r, eval=TRUE, echo=FALSE, cache=TRUE}
head(raw, 2)
```

   
### 3.1 Exploratory analysis 1: the coverages of frequent words.  
  Plot below shows that a few words could cover the majority of English text. On the other hand, there is a long tail of rare words, symbos, foreign languages. Potentially further exploration or modeling could trim the list for efficiency. At this state, We just leave the long tail as it is. 
```{r echo=FALSE, cache=TRUE, eval=TRUE}
?raw
df1 <- raw %>%
        unnest_tokens(word, text) 



df1 <- df1 %>% 
        count(source, word) %>% 
        arrange(desc(n))
# removing profanity words. 
df1 <- df1[!str_detect(df1$word, bad), ]

# Removing numbers. 
df1 <- df1[!str_detect(df1$word, "\\(?[0-9,.]+\\)?"), ]

# Removing white space. 
df1$word <- gsub("\\s", "", df1$word)   

# Removing non-English words. 
df1 <- df1[stri_enc_isascii(df1$word) == TRUE, ]

accu <- cumsum(df1$n)/sum(df1$n)
# Plotting the acucmulative word percentage. 
plot(accu, 
     type = "l", 
     xlab = "Number", 
     ylab = "Accumulated percentage", 
     main = "Accumulated percentage of words")
```

### 3.2 Exploratory analysis 2: term frequency analysis.
```{r echo=FALSE, cache=TRUE}
df1 %>% 
        arrange(desc(n)) %>% 
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        group_by(source) %>%
        top_n(15)%>%
        ungroup()%>%
        ggplot(aes(x = word, y = n, fill = source)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "Count, term frequency") + 
        ggtitle("Top 15 words by count") +
        facet_wrap(~source, ncol = 3, scales = "free") + coord_flip() + 
        scale_fill_brewer(palette = "RdYlBu")
```


### 3.3 Exploratory analysis 3: term frequency-inverse document frequency (tf-idf)analysis. 
  Are there any differences from the three sources: twitter, news and blog? The common words in the corpus is too common to be meaningful. So we can't use a simple frequency model here. Term frequency-inverse document frequency (tf-idf) model could offset those "stop-words" and put more meaningful words on the table. (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
  Plot below: excluding common English "stop-words", we can see the top 15 words from the three sources are quite different. Many "atypical" English words were noted in twitter. 
  The take-homes here: 1) Need all of the three sources in training; 2) English is dynamic, some informal words are active in social media. 
```{r echo=FALSE, cache=TRUE, eval=TRUE}

tfidf <- df1 %>%
        bind_tf_idf(word, source, n)

tfidf %>%
        group_by(source) %>%
        arrange(desc(tf_idf))%>%
        top_n(10) %>%
        ungroup() %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        group_by(source, word) %>%
        top_n(15) %>%
        ungroup()%>%
        ggplot(aes(x = word, y = tf_idf, fill = source)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "tf_idf") + 
        ggtitle("Top 15 words by tf_idf") +
        facet_wrap(~source, ncol = 3, scales = "free") + coord_flip() + 
        scale_fill_brewer(palette = "RdYlBu")
```
  
### 3.4 Exploratory analysis 4: Document-frequency-matrix.   
```{r echo=FALSE}
# Create docoment frequency matrix by tidytext::cast_dfm. 
mydfm <- df1 %>%
        cast_dfm(source, word, n)
# Top 20 feature of the matrix. 
topfeatures(mydfm, 20)

```

  Top 20 words accounts fro 28% of total words!
```{r echo=FALSE}
sum(topfeatures(mydfm, 20))/sum(df1$n)
```


  Word cloud of words with 10000 + times in corpus. 
```{r echo=FALSE}
textplot_wordcloud(mydfm, min.freq = 10000, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

## 4 Milestone conclusions and further steps.
Milestone conclusions: 
1. Potentially this work is system (memory, CPU) demanding; 
2. To avoid system overwhelming, the raw corpus need further trimming or optimization; 
3. tf-idf model could be useful to predict the true meaningfull words; 
4. Common stop words should be included in prediction model, but it might be possible to treat common words as default to minimize the system (memory, CPU) requirment; 

Further steps: 
1. Continue to expore 2gram, 3gram model; 
2. Based on 1gram, 2gram, 3gram or more, build prediction model; 
3. Evaluate the smoothing algorithm. 

